/**
 * LR parser generated by the Syntax tool.
 *
 * To regenerate run:
 *
 *   ./bin/syntax \
 *     --grammar ~/path-to-grammar-file
 *     --mode <parsing-mode>
 *     --output ~/path-to-output-file.js
 *
 * In case of custom tokenizer, one may pass `--custom-tokenizer` option.
 */

'use strict';

let yytext;
let yyleng;
let $$;

const EOF = {
  toString() {
    return '$';
  }
};

const productions = [[-1, 1],
[0, 2, ($1,$2) => { return $$ = {bnf: $2 }; }],
[1, 2, ($1,$2) => { $$ = $1; $$[$2[0]] = $2[1]; }],
[1, 1, ($1) => { $$ = {}; $$[$1[0]] = $1[1]; }],
[2, 4, ($1,$2,$3,$4) => { $$ = [$1, $3]; }],
[3, 1, ($1) => { $$ = yytext; }],
[4, 3, ($1,$2,$3) => { $$ = $1; $$.push($3); }],
[4, 1, ($1) => { $$ = [$1]; }],
[5, 2, ($1,$2) => { $$ = [$1, $2]; }],
[6, 1, ($1) => { $$ = $1; }],
[6, 0, () => { $$ = ''; }],
[7, 2, ($1,$2) => { $$ = $1 + ' ' + $2; }],
[7, 1, ($1) => { $$ = $1; }],
[8, 1, ($1) => { $$ = yytext; }],
[8, 1, ($1) => { $$ = yytext; }],
[9, 1, ($1) => { $$ = yytext; }],
[9, 0, () => { $$ = null; }]];
const table = {"0":{"0":9,"%%":"s1"},"1":{"1":2,"2":10,"3":3,"ID":"s11"},"2":{"2":12,"3":3,"$":"r1","ID":"s11"},"3":{"SPLITTER":"s4"},"4":{"4":5,"5":13,"6":6,"7":7,"8":14,";":"r10","|":"r10","CODE":"r10","ID":"s15","STRING":"s16"},"5":{";":"s17","|":"s8"},"6":{"9":19,"CODE":"s20",";":"r16","|":"r16"},"7":{"8":21,";":"r9","|":"r9","CODE":"r9","ID":"s15","STRING":"s16"},"8":{"5":18,"6":6,"7":7,"8":14,";":"r10","|":"r10","CODE":"r10","ID":"s15","STRING":"s16"},"9":{"$":"acc"},"10":{"ID":"r3","$":"r3"},"11":{"SPLITTER":"r5"},"12":{"ID":"r2","$":"r2"},"13":{";":"r7","|":"r7"},"14":{";":"r12","ID":"r12","|":"r12","STRING":"r12","CODE":"r12"},"15":{";":"r13","ID":"r13","|":"r13","STRING":"r13","CODE":"r13"},"16":{";":"r14","ID":"r14","|":"r14","STRING":"r14","CODE":"r14"},"17":{"ID":"r4","$":"r4"},"18":{";":"r6","|":"r6"},"19":{";":"r8","|":"r8"},"20":{";":"r15","|":"r15"},"21":{";":"r11","ID":"r11","|":"r11","STRING":"r11","CODE":"r11"}};

let stack = [];

function unexpectedToken(token) {
  if (token.value === EOF) {
    unexpectedEndOfInput();
  }
  parseError(`Unexpected token: ${token.value}.`);
}

function unexpectedEndOfInput() {
  parseError(`Unexpected end of input.`);
}

function parseError(message) {
  throw new Error(`Parse error: ${message}`);
}

let tokenizer;
/**
 * Generic tokenizer used by the parser in the Syntax tool.
 *
 * See `--custom-tokinzer` to skip this generation, and use a custom one.
 */

const lexRules = [[/^\/\*(.|\s)*?\*\//, () => {  }],
[/^\s+/, () => {  }],
[/^\{\s*(.*)\s*\}/, () => { yytext = yytext.slice(1, -1).trim(); return 'CODE' }],
[/^[a-zA-Z][a-zA-Z0-9_-]*/, () => { return 'ID' }],
[/^(?:->|:)/, () => { return 'SPLITTER' }],
[/^;/, () => { return ';' }],
[/^\|/, () => { return '|' }],
[/^\{/, () => { return '{' }],
[/^\}/, () => { return '}' }],
[/^%%/, () => { return '%%' }],
[/^(?:"|')([^"']*)(?:"|')/, () => { return 'STRING' }]];

tokenizer = {
  initString(string) {
    this._string = string + EOF;
    this._cursor = 0;
    return this;
  },

  getNextToken() {
    if (!this.hasMoreTokens()) {
      return null;
    } else if (this.isEOF()) {
      this._cursor++;
      return {
        type: EOF,
        value: EOF,
      };
    }

    let string = this._string.slice(this._cursor);

    for (let i = 0; i < lexRules.length; i++) {
      let lexRule = lexRules[i];
      let matched = this._match(string, lexRule[0]);
      if (matched) {
        yytext = matched;
        yyleng = yytext.length;
        let token = lexRule[1]();

        if (!token) {
          return this.getNextToken();
        }

        return {
          type: token,
          value: yytext,
        };
      }
    }

    throw new Error(`Unexpected token: "${string[0]}".`);
  },

  isEOF() {
    return this._string[this._cursor] === EOF.toString();
  },

  hasMoreTokens() {
    return this._cursor < this._string.length;
  },

  _match(string, regexp) {
    let matched = string.match(regexp);
    if (matched) {
      this._cursor += matched[0].length;
      return matched[0];
    }
    return null;
  },
};

const LRParser = {
  parse(string) {
    if (!tokenizer) {
      throw new Error(`Tokenizer instance wasn't specified.`);
    }

    tokenizer.initString(string);

    stack = [];

    stack.push(0);

    let token = tokenizer.getNextToken();
    let shiftedToken = null;

    do {
      if (!token) {
        unexpectedEndOfInput();
      }

      let state = stack[stack.length - 1];
      let column = token.type;
      let entry = table[state][column];

      if (!entry) {
        unexpectedToken(token);
      }

      if (entry[0] === 's') {
        stack.push(
          {symbol: token.type, semanticValue: token.value},
          Number(entry.slice(1))
        );
        shiftedToken = token;
        token = tokenizer.getNextToken();
      } else if (entry[0] === 'r') {
        let productionNumber = entry.slice(1);
        let production = productions[productionNumber];
        let hasSemanticAction = typeof production[2] === 'function';
        let semanticActionArgs = hasSemanticAction ? [] : null;

        if (production[1] !== 0) {
          let rhsLengh = production[1];
          while (rhsLengh--) {
            stack.pop();
            let stackEntry = stack.pop();

            if (hasSemanticAction) {
              semanticActionArgs.unshift(stackEntry.semanticValue);
            }
          }
        }

        let reduceStackEntry = {symbol: production[0]};

        if (hasSemanticAction) {
          yytext = shiftedToken ? shiftedToken.value : null;
          yyleng = shiftedToken ? shiftedToken.value.length : null;

          production[2](...semanticActionArgs);
          reduceStackEntry.semanticValue = $$;
        }

        stack.push(
          reduceStackEntry,
          table[stack[stack.length - 1]][production[0]]
        );
      } else if (entry === 'acc') {
        stack.pop();
        let parsed = stack.pop();

        if (stack.length !== 1 ||
            stack[0] !== 0 ||
            tokenizer.hasMoreTokens()) {
          unexpectedToken(token);
        }

        if (parsed.hasOwnProperty('semanticValue')) {
          return parsed.semanticValue;
        }

        return true;
      }

    } while (tokenizer.hasMoreTokens() || stack.length > 1);
  },

  setTokenizer(customTokenizer) {
    tokenizer = customTokenizer;
    return this;
  },

  getTokenizer() {
    return tokenizer;
  },

};

module.exports = LRParser;
